{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iliJuq6afHjz",
   "metadata": {
    "id": "iliJuq6afHjz"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {
    "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {
    "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a"
   },
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, average_precision_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {
    "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954"
   },
   "source": [
    "## Set Arguments & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {
    "id": "8f9c4250-2257-404f-941d-58eff1e9eb38"
   },
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {
    "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "### 데이터 디렉토리 설정 ###\n",
    "DATA_DIR= 'data'\n",
    "NUM_CLS = 2\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 34\n",
    "LEARNING_RATE = 0.0005\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "INPUT_SHAPE = 128\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db9154a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7db9154a",
    "outputId": "8176207e-90b5-4c02-8c7e-9451fb461984"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check device\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {
    "id": "d44807b0-7788-49ec-aff2-c756e4513c5e"
   },
   "source": [
    "# Define Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {
    "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05"
   },
   "source": [
    "## Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {
    "id": "04642777-c2e0-439b-9692-f6c571a86521"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Dataset split\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:578]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[578:]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['COVID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27520-c82c-4ec8-ae0b-119a79167f09",
   "metadata": {
    "id": "61b27520-c82c-4ec8-ae0b-119a79167f09"
   },
   "source": [
    "# Define Model(VGG-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcea77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "\n",
    "\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, NUM_CLS):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.vgg = vgg19(pretrained=False)\n",
    "        self.features_conv = self.vgg.features\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(8192, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, NUM_CLS),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056905-1f77-4579-a260-07bb1056f6db",
   "metadata": {
    "id": "1d056905-1f77-4579-a260-07bb1056f6db"
   },
   "source": [
    "## Utils\n",
    "### EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87",
   "metadata": {
    "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87"
   },
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가, 감소 시 0으로 리셋\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "        self.save_model = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            return None\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "        \n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {
    "id": "1aaffd8d-b025-42c1-8dd8-69529487389e"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {
    "id": "5faaac1b-64c3-4659-82de-d4309502f29a"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" epoch에 대한 학습 및 검증 절차 정의\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, model, device, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, F1-Macro: {f1}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, F1-Macro: {f1}'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {
    "id": "e2aca506-d168-4c9f-8eca-5cdecb122961"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {
    "id": "33678d90-a254-48d5-bf09-2a817eeafea3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    \n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    f1 = f1_score(y_answer, y_pred, average='macro')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
   "metadata": {
    "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
    "tags": []
   },
   "source": [
    "## Train\n",
    "### 학습을 위한 객체 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19610a4-ad7c-44a0-80cd-9734b5015100",
   "metadata": {
    "id": "b19610a4-ad7c-44a0-80cd-9734b5015100",
    "tags": []
   },
   "source": [
    "#### Load Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
    "outputId": "8c4a6a6e-7f32-49dd-e079-304a9bb83983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n",
      "Train set samples: 578 Val set samples: 68\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8dae0-8e32-4ac0-a585-858a7095d2a4",
   "metadata": {
    "id": "3bb8dae0-8e32-4ac0-a585-858a7095d2a4"
   },
   "source": [
    "#### Load model and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb4d52e1-752a-40d5-9b34-c06d3dbdd45a",
   "metadata": {
    "id": "cb4d52e1-752a-40d5-9b34-c06d3dbdd45a"
   },
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = VGG19(NUM_CLS).to(DEVICE)\n",
    "\n",
    "# # Save Initial Model\n",
    "# torch.save(model.state_dict(), 'initial.pt')\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(loss_fn, model, DEVICE, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b881024-3921-4c2c-b9ec-e6b23c4f5ad2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b881024-3921-4c2c-b9ec-e6b23c4f5ad2",
    "outputId": "5dc21f20-ee28-40d6-8c76-40cd81fd8932",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG19(\n",
       "  (vgg): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): ReLU(inplace=True)\n",
       "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (33): ReLU(inplace=True)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (features_conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa8aef-b984-4133-b6b2-e1c85900f724",
   "metadata": {
    "id": "d5aa8aef-b984-4133-b6b2-e1c85900f724"
   },
   "source": [
    "### epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcc35f70-25fc-48e1-92f8-8633b3b8be80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "dcc35f70-25fc-48e1-92f8-8633b3b8be80",
    "outputId": "3ac349b2-c2ca-460f-b8f1-0b389793a804"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 6.00 GiB total capacity; 4.08 GiB already allocated; 0 bytes free; 4.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4940\\2285660684.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4940\\3664607097.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\practice\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\practice\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 6.00 GiB total capacity; 4.08 GiB already allocated; 0 bytes free; 4.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if early_stopper.save_model:\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, 'best.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53514a-e83f-4795-9589-640f26cc2993",
   "metadata": {
    "id": "fe53514a-e83f-4795-9589-640f26cc2993"
   },
   "source": [
    "## Inference\n",
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3",
   "metadata": {
    "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3"
   },
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = 'best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda1c64",
   "metadata": {
    "id": "8fda1c64"
   },
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49fe293",
   "metadata": {
    "id": "a49fe293"
   },
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def get_clf_eval(y_actual, y_pred):\n",
    "    accuracy = accuracy_score(y_actual, y_pred)\n",
    "    precision = precision_score(y_actual, y_pred)\n",
    "    recall = recall_score(y_actual, y_pred)\n",
    "    PR_AUC = average_precision_score(y_actual, y_pred)\n",
    "    AUC = roc_auc_score(y_actual, y_pred)\n",
    "    F1 = f1_score(y_actual, y_pred, average='macro')\n",
    "    print('\\n정확도: {:.4f}'.format(accuracy))\n",
    "    print('정밀도: {:.4f}'.format(precision))\n",
    "    print('재현율: {:.4f}'.format(recall))\n",
    "    print('AUC: {:.4f}'.format(AUC))\n",
    "    print('F1: {:.4f}'.format(F1))\n",
    "    \n",
    "    sns.heatmap(confusion_matrix(y_actual, y_pred), annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dda53410",
   "metadata": {
    "id": "dda53410",
    "outputId": "da30df66-d4d6-4fc7-bafd-3d5bea06ffe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:06,  2.47it/s]\n",
      "2it [00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "train_actual = []\n",
    "validation_actual = []\n",
    "train_pred_lst = []\n",
    "validation_pred_lst = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, label) in tqdm(enumerate(train_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        train_actual += (list(label.numpy()))\n",
    "        train_pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, label) in tqdm(enumerate(validation_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        validation_actual += (list(label.numpy()))\n",
    "        validation_pred_lst.extend(pred.argmax(dim=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa91de4",
   "metadata": {
    "id": "afa91de4",
    "outputId": "816d80ba-ecb9-4812-de28-7c8febbbf8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정확도: 1.0000\n",
      "정밀도: 1.0000\n",
      "재현율: 1.0000\n",
      "AUC: 1.0000\n",
      "F1: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_clf_eval(train_actual, train_pred_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6f22070",
   "metadata": {
    "id": "c6f22070",
    "outputId": "b116d087-0727-4428-8802-4e0747498ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정확도: 0.7794\n",
      "정밀도: 0.8800\n",
      "재현율: 0.6471\n",
      "AUC: 0.7794\n",
      "F1: 0.7755\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_clf_eval(validation_actual, validation_pred_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {
    "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5",
   "metadata": {
    "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'sample_submission.csv'))\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'test',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {
    "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
    "outputId": "88bd6c8e-ddbe-41a9-ee4d-19f2e2d03282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efd72b-172d-4e34-a1dd-65ed8c745b58",
   "metadata": {
    "id": "53efd72b-172d-4e34-a1dd-65ed8c745b58"
   },
   "source": [
    "### 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
   "metadata": {
    "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
    "outputId": "f878a101-6e12-4ace-bc02-dd3a20cc0512",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9998e-01, 2.0674e-05],\n",
      "        [9.9914e-01, 8.6014e-04],\n",
      "        [9.9438e-01, 5.6196e-03],\n",
      "        [9.2767e-02, 9.0723e-01],\n",
      "        [9.8741e-01, 1.2592e-02],\n",
      "        [3.1749e-06, 1.0000e+00],\n",
      "        [1.6044e-03, 9.9840e-01],\n",
      "        [7.8440e-02, 9.2156e-01],\n",
      "        [1.3884e-01, 8.6116e-01],\n",
      "        [4.2473e-07, 1.0000e+00],\n",
      "        [9.9963e-01, 3.7104e-04],\n",
      "        [9.9928e-01, 7.1690e-04],\n",
      "        [9.9998e-01, 2.2702e-05],\n",
      "        [9.9998e-01, 1.6049e-05],\n",
      "        [9.9974e-01, 2.5918e-04],\n",
      "        [1.0000e+00, 4.9491e-07],\n",
      "        [8.3480e-01, 1.6520e-01],\n",
      "        [1.0000e+00, 1.7216e-07],\n",
      "        [9.7831e-01, 2.1690e-02],\n",
      "        [9.9873e-01, 1.2740e-03],\n",
      "        [1.0000e+00, 1.7932e-09],\n",
      "        [9.9994e-01, 5.6686e-05],\n",
      "        [9.8775e-01, 1.2249e-02],\n",
      "        [9.9995e-01, 5.3763e-05],\n",
      "        [1.0452e-02, 9.8955e-01],\n",
      "        [9.9999e-01, 9.7956e-06],\n",
      "        [9.9991e-01, 9.4280e-05],\n",
      "        [8.0732e-01, 1.9268e-01],\n",
      "        [6.0370e-08, 1.0000e+00],\n",
      "        [1.4762e-01, 8.5238e-01],\n",
      "        [1.7816e-03, 9.9822e-01],\n",
      "        [8.0308e-05, 9.9992e-01],\n",
      "        [1.0357e-01, 8.9643e-01],\n",
      "        [9.9991e-01, 9.0822e-05]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:01,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9963e-01, 3.6882e-04],\n",
      "        [6.5627e-02, 9.3437e-01],\n",
      "        [2.1291e-02, 9.7871e-01],\n",
      "        [6.0814e-01, 3.9186e-01],\n",
      "        [3.9961e-07, 1.0000e+00],\n",
      "        [9.9955e-01, 4.5361e-04],\n",
      "        [3.8972e-05, 9.9996e-01],\n",
      "        [2.5869e-03, 9.9741e-01],\n",
      "        [1.0000e+00, 1.7002e-06],\n",
      "        [4.5400e-08, 1.0000e+00],\n",
      "        [6.8614e-02, 9.3139e-01],\n",
      "        [6.7181e-03, 9.9328e-01],\n",
      "        [8.7165e-02, 9.1283e-01],\n",
      "        [1.0000e+00, 4.5522e-07],\n",
      "        [9.8610e-01, 1.3903e-02],\n",
      "        [9.9994e-01, 5.8375e-05],\n",
      "        [6.1594e-06, 9.9999e-01],\n",
      "        [3.6133e-06, 1.0000e+00],\n",
      "        [6.0641e-01, 3.9359e-01],\n",
      "        [9.9894e-01, 1.0645e-03],\n",
      "        [7.5678e-06, 9.9999e-01],\n",
      "        [7.8444e-01, 2.1556e-01],\n",
      "        [4.5900e-01, 5.4100e-01],\n",
      "        [9.0825e-05, 9.9991e-01],\n",
      "        [9.9986e-01, 1.4094e-04],\n",
      "        [9.9990e-01, 1.0146e-04],\n",
      "        [2.2185e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 5.6797e-06],\n",
      "        [1.0000e+00, 1.7735e-07],\n",
      "        [1.0000e+00, 2.8314e-09],\n",
      "        [4.7802e-05, 9.9995e-01],\n",
      "        [7.0114e-06, 9.9999e-01],\n",
      "        [2.1813e-01, 7.8187e-01],\n",
      "        [9.2602e-01, 7.3976e-02]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7424e-03, 9.9726e-01],\n",
      "        [9.9998e-01, 1.6729e-05],\n",
      "        [9.9999e-01, 1.3921e-05],\n",
      "        [9.9700e-01, 3.0021e-03],\n",
      "        [1.0000e+00, 4.8190e-07],\n",
      "        [9.9999e-01, 1.0527e-05],\n",
      "        [3.2867e-03, 9.9671e-01],\n",
      "        [1.0000e+00, 4.7472e-06],\n",
      "        [1.0000e+00, 1.4647e-08],\n",
      "        [9.9993e-01, 7.4853e-05],\n",
      "        [3.4805e-03, 9.9652e-01],\n",
      "        [1.1164e-05, 9.9999e-01],\n",
      "        [1.4302e-03, 9.9857e-01],\n",
      "        [2.9466e-03, 9.9705e-01],\n",
      "        [9.9999e-01, 1.1304e-05],\n",
      "        [5.3147e-05, 9.9995e-01],\n",
      "        [1.8100e-02, 9.8190e-01],\n",
      "        [7.5305e-02, 9.2470e-01],\n",
      "        [9.9997e-01, 2.9893e-05],\n",
      "        [5.4421e-03, 9.9456e-01],\n",
      "        [9.9249e-01, 7.5141e-03],\n",
      "        [6.9805e-01, 3.0195e-01],\n",
      "        [9.9651e-01, 3.4914e-03],\n",
      "        [9.9963e-01, 3.7260e-04],\n",
      "        [2.8639e-04, 9.9971e-01],\n",
      "        [3.7308e-05, 9.9996e-01],\n",
      "        [1.0000e+00, 3.5911e-08],\n",
      "        [9.8364e-01, 1.6355e-02],\n",
      "        [1.8799e-06, 1.0000e+00],\n",
      "        [2.4270e-04, 9.9976e-01],\n",
      "        [8.9259e-01, 1.0741e-01],\n",
      "        [1.0077e-05, 9.9999e-01]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        print(pred)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst.extend(pred[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056169d1-64a8-4b81-8daf-722b029cf2b9",
   "metadata": {
    "id": "056169d1-64a8-4b81-8daf-722b029cf2b9"
   },
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff",
   "metadata": {
    "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':file_lst, 'COVID':pred_lst})\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('prediction(VGG19).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5b7ae-2810-42c4-9335-ae1c6b6e2746",
   "metadata": {
    "id": "03a5b7ae-2810-42c4-9335-ae1c6b6e2746"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG-19.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
